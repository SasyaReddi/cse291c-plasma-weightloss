{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh_FkgR00zpY"
   },
   "source": [
    "### <font color='blue'>**Data Exploration + XGBoost starter model**\n",
    "    Pulled from Eric's notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-103415c14a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "modelA2 = XGBClassifier(use_label_encoder=False, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, pep_df, min_timepoints_per_patient=2):\n",
    "        '''\n",
    "        pep_df should be the pep_reduced_intensity_df we created above\n",
    "        This dataset will only include patients with at least `min_timepoints_per_patient` timepoints\n",
    "        '''\n",
    "        self.pep_df = pep_df\n",
    "        \n",
    "        self.data = {} # patient id -> [array of timepoint vectors]\n",
    "        for patient_idx in range(1, 58+1):\n",
    "            patient_timepoint_vectors = []\n",
    "            for timepoint in range(1, 7+1):\n",
    "                try:\n",
    "                    # We replace all nans with zeros using np.nan_to_num\n",
    "                    patient_timepoint_vector = np.nan_to_num(self.pep_df[f\"Patient_{patient_idx:02d}.Timepoint_{timepoint:01d}\"].values)\n",
    "                    patient_timepoint_vectors.append(patient_timepoint_vector)\n",
    "                except:\n",
    "                    # This patient timepoint doesnt exist in the data\n",
    "                    pass\n",
    "            \n",
    "            # If this patient had at least `min_timepoints_per_patient` timepoints, then include this patient id\n",
    "            if len(patient_timepoint_vectors) >= min_timepoints_per_patient:\n",
    "                self.data[patient_idx] = patient_timepoint_vectors\n",
    "                \n",
    "    @staticmethod\n",
    "    def prepare_df(filepath):\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "        \n",
    "        # only keep the Patient_x.Timepoint_y formatted columns\n",
    "        df = df.iloc[:, :667]\n",
    "        \n",
    "        # Remove all _unmod columns\n",
    "        df = df[df.columns.drop(list(df.filter(regex='.\\_unmod')))]\n",
    "        \n",
    "        # Convert the string abundance numbers into ints for all patient timepoint cols\n",
    "        def convert_to_number(val):\n",
    "            if isinstance(val, str):\n",
    "                return int(val.replace(\",\",\"\").strip())\n",
    "            return val\n",
    "\n",
    "        for patient_timepoint_col in df.columns.values[32:]:\n",
    "            df[patient_timepoint_col] = df[patient_timepoint_col].apply(convert_to_number)\n",
    "            df[patient_timepoint_col] = df[patient_timepoint_col].astype(float)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, filepath, *args, **kwargs):\n",
    "        pip_df = cls.prepare_df(filepath)\n",
    "        return cls(pip_df, *args, **kwargs)\n",
    "        \n",
    "                \n",
    "    def data_generator(self, patient_ids, num_samples, peptide_indices=None, same_patient_pair_probability=0.5):\n",
    "        \"\"\"\n",
    "        This will return a generator that will yield `num_samples` sample pairs of peptide vectors \n",
    "        (limited to only the `patient_ids` given, and the `peptide_indices` given (if none is given, then all peptides will be included))\n",
    "        such that, `same_patient_pair_probability` of the time, the pair of vectors will be from the same patient (but different time points)\n",
    "                \n",
    "        Data samples yielded by the returned generator will be of the form:\n",
    "        {\n",
    "            \"first_patient_idx\" : ...,\n",
    "            \"first_patient_timepoint_vector\" : ...,\n",
    "            \"second_patient_idx\" : ...,\n",
    "            \"second_patient_timepoint_vector\" : ...,\n",
    "            \"is_same_patient\" : True/False\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        assert same_patient_pair_probability is not None\n",
    "        \n",
    "        # First ensure all of the given patient ids are in out prepared dataset\n",
    "        assert all(pid in self.data for pid in patient_ids), \"Not all of the given patient ids are in our dataset\"\n",
    "        \n",
    "        def data_gen():\n",
    "            # This generator is used when same_patient_pair_probability is NOT None\n",
    "            for _ in range(num_samples):\n",
    "                \n",
    "                # pick a random first patient id\n",
    "                first_patient_id = random.choice(patient_ids)\n",
    "                \n",
    "                # pick a random timepoint vector for this first patient id\n",
    "                first_patient_timepoint_vector = random.choice(self.data[first_patient_id])\n",
    "                \n",
    "                # decide if the second patient in our pair should be the same as the first\n",
    "                is_second_patient_same = random.random() <= same_patient_pair_probability\n",
    "                \n",
    "                if is_second_patient_same:\n",
    "                    second_patient_id = first_patient_id\n",
    "                else:\n",
    "                    second_patient_id = random.choice([pid for pid in patient_ids if pid != first_patient_id])\n",
    "                \n",
    "                # Note: In this approach, it is possible that, when the second patient = first patient,\n",
    "                # They both return the same timepoint vector. but that should be okay, since this will help the model learn\n",
    "                # to detect identical inputs, especially when the input to the model is the concatenated vector\n",
    "                second_patient_timepoint_vector = random.choice(self.data[second_patient_id])\n",
    "                \n",
    "                # if peptide_indices is given, filter the vectors to only include the `peptide_indices` peptides\n",
    "                if peptide_indices:\n",
    "                    first_patient_timepoint_vector = first_patient_timepoint_vector[peptide_indices]\n",
    "                    second_patient_timepoint_vector = second_patient_timepoint_vector[peptide_indices]\n",
    "                \n",
    "                yield {\n",
    "                    \"first_patient_id\" : first_patient_id,\n",
    "                    \"first_patient_timepoint_vector\" : first_patient_timepoint_vector,\n",
    "                    \"second_patient_id\" : second_patient_id,\n",
    "                    \"second_patient_timepoint_vector\" : second_patient_timepoint_vector,\n",
    "                    \"is_same_patient\" : first_patient_id == second_patient_id\n",
    "                }\n",
    "\n",
    "        return data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./Data\"\n",
    "DATA_TSV_FILEPATH = os.path.join(DATA_DIR, \"peptidoforms_intensity\", \"data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish all random seeds\n",
    "random.seed(2021)\n",
    "\n",
    "# TODO: other libraries should be seeded here, like sklearn, torch, xgboost, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>**Creating Main Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptidoforms_intensity_dataset = Dataset.from_file(DATA_TSV_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(40921, 350)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptidoforms_intensity_df = peptidoforms_intensity_dataset.pep_df\n",
    "peptidoforms_intensity_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6Z8H_TkcxZE"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFUA7k1jGkcH"
   },
   "source": [
    "### <font color='blue'>**Creating Train/Val/Test Datasets**\n",
    "    \n",
    "Train/val/test datasets will enforce the `same_patient_pair_probability = 0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we don't select a particular subset of peptides, and just use all of them.\n",
    "selected_peptide_indices = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_patient_pair_probability = 0.5\n",
    "val_same_patient_pair_probability = 0.5\n",
    "test_same_patient_pair_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 1k samples for train/val/test finishes fast, but may not be accurate\n",
    "# so we should train with more samples (~100K train/val/test) with colab/gpu later, which would take more time\n",
    "num_train_samples = 10000\n",
    "num_val_samples = 10000\n",
    "num_test_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "52"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_patient_ids = list(peptidoforms_intensity_dataset.data.keys())\n",
    "num_patient_ids = len(all_patient_ids)\n",
    "num_patient_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(31, 10, 11)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(all_patient_ids)\n",
    "train_patient_ids, val_patient_ids, test_patient_ids = np.split(all_patient_ids, \n",
    "                                                                [int(train_ratio * num_patient_ids), \n",
    "                                                                 int((train_ratio + val_ratio) * num_patient_ids)\n",
    "                                                                ])\n",
    "train_patient_ids = train_patient_ids.tolist()\n",
    "val_patient_ids = val_patient_ids.tolist()\n",
    "test_patient_ids = test_patient_ids.tolist()\n",
    "\n",
    "len(train_patient_ids), len(val_patient_ids), len(test_patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_arrays_from_generator(gen):\n",
    "    \"\"\"\n",
    "    returns (concatX, absdiffX Y)\n",
    "    \n",
    "    concatX is the two input feature vectors concatenated together\n",
    "    absdiffX is the abs difference between the two input feature vectors\n",
    "    Y is always 1/0 if the vectors are from the same patient or not\n",
    "    \"\"\"    \n",
    "    concatX, absdiffX, Y = [], [], []\n",
    "    \n",
    "    for data_input in tqdm(gen):\n",
    "        concatX.append(np.hstack((data_input['first_patient_timepoint_vector'], data_input['second_patient_timepoint_vector'])).astype(int))\n",
    "        absdiffX.append(np.abs(data_input['first_patient_timepoint_vector'] - data_input['second_patient_timepoint_vector']).astype(int))\n",
    "        Y.append(1 if data_input['is_same_patient'] else 0)\n",
    "    \n",
    "    return np.array(concatX), np.array(absdiffX), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "10000it [00:10, 979.81it/s]\n10000it [00:10, 951.29it/s]\n10000it [00:09, 1064.39it/s]\n"
    }
   ],
   "source": [
    "train_data_gen = peptidoforms_intensity_dataset.data_generator(\n",
    "    patient_ids=train_patient_ids, \n",
    "    num_samples=num_train_samples, \n",
    "    peptide_indices=selected_peptide_indices, \n",
    "    same_patient_pair_probability=train_same_patient_pair_probability)\n",
    "concatTrainX, absDiffTrainX, trainY = create_data_arrays_from_generator(train_data_gen)\n",
    "\n",
    "val_data_gen = peptidoforms_intensity_dataset.data_generator(\n",
    "    patient_ids=val_patient_ids, \n",
    "    num_samples=num_val_samples, \n",
    "    peptide_indices=selected_peptide_indices, \n",
    "    same_patient_pair_probability=val_same_patient_pair_probability)\n",
    "concatValX, absDiffValX, valY = create_data_arrays_from_generator(val_data_gen)\n",
    "\n",
    "test_data_gen = peptidoforms_intensity_dataset.data_generator(\n",
    "    patient_ids=test_patient_ids, \n",
    "    num_samples=num_test_samples, \n",
    "    peptide_indices=selected_peptide_indices, \n",
    "    same_patient_pair_probability=test_same_patient_pair_probability)\n",
    "concatTestX, absDiffTestX, testY = create_data_arrays_from_generator(test_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    5005\n1    4995\ndtype: int64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check that the labels are in a rough 50-50 distribution\n",
    "pd.Series(trainY).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1    5079\n0    4921\ndtype: int64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check that the labels are in a rough 50-50 distribution\n",
    "pd.Series(valY).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1    5033\n0    4967\ndtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check that the labels are in a rough 50-50 distribution\n",
    "pd.Series(testY).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7AWhg0RJXeQ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJAvFabadQuR"
   },
   "source": [
    "### <font color='blue'>**XGBoost model variants**</font>\n",
    "\n",
    "The different variants we will be training are the following:\n",
    "\n",
    "First, train 2 models where one takes in concatenated features vs. the other takes in the absolute value difference of the input feature vectors. Let the better of these 2 models (based on the lowest val loss achieved) be called model A.\n",
    "\n",
    "Using model A, get its important feature weights, and try to train the same model architecture but only using the top k features (where k = 1, 10, 100, num_nonzeroweight_peptides_from_modelA).\n",
    "\n",
    "Finally, using model A, also train only using peptides present in at most N patients for N = (5, 10, 20, 40).\n",
    "\n",
    "---\n",
    "\n",
    "**Sanity check:** Initial log loss should be around $0.69$, since $-ln(0.5) = 0.6931471806$ and there is a $0.5$ probability of same/different patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "sklearn needs to be installed in order to use this module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8bbbcb1789b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_label_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodelA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_label_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objective, use_label_encoder, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary:logistic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_label_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_label_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_label_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_depth, learning_rate, n_estimators, verbosity, objective, booster, tree_method, n_jobs, gamma, min_child_weight, max_delta_step, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, base_score, random_state, missing, num_parallel_tree, monotone_constraints, interaction_constraints, importance_type, gpu_id, validate_parameters, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     ):\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSKLEARN_INSTALLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             raise XGBoostError(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0;34m\"sklearn needs to be installed in order to use this module\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             )\n",
      "\u001b[0;31mXGBoostError\u001b[0m: sklearn needs to be installed in order to use this module"
     ]
    }
   ],
   "source": [
    "modelA1 = XGBClassifier(use_label_encoder=False, max_depth=3)\n",
    "modelA2 = XGBClassifier(use_label_encoder=False, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained on concatenated vectors\n",
    "print(\"Training model A1: concatenated input feature vectors\")\n",
    "modelA1.fit(concatTrainX, trainY, eval_metric=\"logloss\", eval_set=[(concatValX, valY)], verbose=True)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# trained on concatenated vectors\n",
    "print(\"Training model A2: abs difference of input feature vectors\")\n",
    "modelA2.fit(absDiffTrainX, trainY, eval_metric=\"logloss\", eval_set=[(absDiffValX, valY)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save models\n",
    "modelA1.save_model('xgboost_modelA1.json')\n",
    "modelA2.save_model('xgboost_modelA2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A variants - metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_metrics(model, model_input_format, mode, peptide_indices=None):\n",
    "    assert model_input_format in (\"concat\", \"abs_diff\")\n",
    "    assert mode in (\"train\", \"val\", \"test\")\n",
    "    \n",
    "    if peptide_indices is None:\n",
    "        x_s = {\n",
    "            \"train\" : {\n",
    "                \"concat\" : concatTrainX,\n",
    "                \"abs_diff\" : absDiffTrainX\n",
    "            },\n",
    "            \"val\" : {\n",
    "                \"concat\" : concatValX,\n",
    "                \"abs_diff\" : absDiffValX\n",
    "            },\n",
    "            \"test\" : {\n",
    "                \"concat\" : concatTestX,\n",
    "                \"abs_diff\" : absDiffTestX\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        x_s = {\n",
    "            \"train\" : {\n",
    "                \"concat\" : concatTrainX,\n",
    "                \"abs_diff\" : absDiffTrainX[:,peptide_indices]\n",
    "            },\n",
    "            \"val\" : {\n",
    "                \"concat\" : concatValX,\n",
    "                \"abs_diff\" : absDiffValX[:,peptide_indices]\n",
    "            },\n",
    "            \"test\" : {\n",
    "                \"concat\" : concatTestX,\n",
    "                \"abs_diff\" : absDiffTestX[:,peptide_indices]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    y_s = {\n",
    "        \"train\" : trainY,\n",
    "        \"val\" : valY,\n",
    "        \"test\" : testY\n",
    "    }\n",
    "    \n",
    "    x = x_s[mode][model_input_format]\n",
    "    y = y_s[mode]\n",
    "        \n",
    "    pred_scores = model.predict(x)\n",
    "    preds = [round(pred_score) for pred_score in pred_scores]\n",
    "    \n",
    "    metrics(y, preds)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    \n",
    "    print (f\"{mode} accuracy: {acc}\")\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y, pred_scores)\n",
    "    \n",
    "    # plotting below taken from: \n",
    "    # https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n",
    "    \n",
    "    # calculate the no skill line as the proportion of the positive class\n",
    "    no_skill = len(y[y==0]) / len(y)\n",
    "    \n",
    "    # plot the no skill precision-recall curve\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='Trivial model')\n",
    "    \n",
    "    # plot the model precision-recall curve\n",
    "    plt.plot(recall, precision, marker='.', label='XGBoost model')\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_metrics(modelA1, \"concat\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_metrics(modelA1, \"concat\", \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_metrics(modelA1, \"concat\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_metrics(modelA2, \"abs_diff\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_metrics(modelA2, \"abs_diff\", \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_metrics(modelA2, \"abs_diff\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model A2 using only the top k peptides (k = 1, 5, 10, 25, 50, num_peptides_with_nonzero_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = modelA2.feature_importances_\n",
    "np.save(\"modelA2_feature_importances.npy\", ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_peptides_with_nonzero_weight = len(ft[ft > 0]) # count number of peptides with a non-zero weight\n",
    "num_peptides_with_nonzero_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_peptides = len(peptidoforms_intensity_df)\n",
    "assert num_unique_peptides == len(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_peptidoform_indices = ft.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get some of the most important (top 10) peptide indices\n",
    "top_peptidoform_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights of the top 10 most important peptides\n",
    "ft[top_peptidoform_indices[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_peptide_indices(X, peptide_indices):\n",
    "    # use only the given peptide indices from the given X matrix, where X is a list of all the absolute difference vectors\n",
    "    # since this model format performed the best, when compared to the concatenated vector representation\n",
    "    return X[:,peptide_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model_on_top_k_peptides(k_values=(1, 5, 10, 25, 50, num_peptides_with_nonzero_weight)):\n",
    "    \n",
    "    topkpeptide_models = []\n",
    "    for k in k_values:\n",
    "        print (\"=\"*50)\n",
    "        print (f\"K value: {k}\")\n",
    "        print (\"=\"*50)\n",
    "        \n",
    "        k_trainX = isolate_peptide_indices(absDiffTrainX, top_peptidoform_indices[:k])\n",
    "        k_trainY = trainY\n",
    "        \n",
    "        k_valX = isolate_peptide_indices(absDiffValX, top_peptidoform_indices[:k])\n",
    "        k_valY = valY\n",
    "        \n",
    "        topk_model = XGBClassifier(use_label_encoder=False, max_depth=3)\n",
    "        topk_model.fit(k_trainX, k_trainY, eval_metric=\"logloss\", eval_set=[(k_valX, k_valY)], verbose=True)\n",
    "        topk_model.save_model(f\"xgboost_modelA2_top{k}_peptides.json\")\n",
    "        topkpeptide_models.append(topk_model)\n",
    "        \n",
    "        print_model_metrics(topk_model, \"abs_diff\", \"train\", peptide_indices=top_peptidoform_indices[:k])\n",
    "        print_model_metrics(topk_model, \"abs_diff\", \"val\", peptide_indices=top_peptidoform_indices[:k])\n",
    "        print_model_metrics(topk_model, \"abs_diff\", \"test\", peptide_indices=top_peptidoform_indices[:k])\n",
    "\n",
    "        print (\"\\n\\n\")\n",
    "    \n",
    "    return topkpeptide_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = try_model_on_top_k_peptides()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>**Important peptides analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis is weight and y-axis is count\n",
    "# this shows that very, very few peptides have a significant weight (ie., > 0.035)\n",
    "pd.Series(ft[ft > 0]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the first half of the features and the second half are semantically the same, \n",
    "# we have to % by the number of unique peptides (ie., the size of the first half of each combined feature vector)\n",
    "# to get the unique set of peptides that are deemed important\n",
    "important_peptide_indices = np.where(ft > 0)[0]\n",
    "len(important_peptide_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patients_peptide_is_present_in(peptide_index):\n",
    "    present_patient_timepoints = peptidoforms_intensity_df.iloc[peptide_index, 32:].dropna().index\n",
    "    \n",
    "    patients = set()\n",
    "    for pt in present_patient_timepoints:\n",
    "        spltidx = pt.index(\".Timepoint_\")\n",
    "        patient_id = int(pt[:spltidx].replace(\"Patient_\", \"\"))\n",
    "        patients.add(patient_id)\n",
    "    \n",
    "    return patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for peptide_index in top_k_peptidoform_indices:\n",
    "    print (f\"Peptide {peptide_index}, weight: {ft[peptide_index]}\")\n",
    "    print (f\"{len(get_patients_peptide_is_present_in(peptide_index))} patients have this peptide\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df = pd.read_csv(DATA_TSV_FILEPATH, sep=\"\\t\")\n",
    "assert original_data_df.shape[0] == peptidoforms_intensity_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df = original_data_df.iloc[important_peptide_indices]\n",
    "original_data_df['model_weight'] = ft[important_peptide_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df.to_csv(\"important_peptidoforms_selected_by_xgboost_all_columns.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_peptidoforms_intensity_df = peptidoforms_intensity_df.iloc[important_peptide_indices]\n",
    "export_peptidoforms_intensity_df['model_weight'] = ft[important_peptide_indices]\n",
    "export_peptidoforms_intensity_df.to_csv(\"important_peptidoforms_selected_by_xgboost.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Any necessary code for Results section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_peptidoform_indices_and_weights = list(zip(important_peptide_indices, ft[important_peptide_indices]))\n",
    "sorted(top_peptidoform_indices_and_weights, key = lambda item: item[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sasya (+PCA) - MLApproach.ipynb",
   "provenance": [
    {
     "file_id": "12_DObUJrtJgTxoDAO0EMkmFffF00QZW-",
     "timestamp": 1620702652816
    },
    {
     "file_id": "1pNQiIjpJYOh77_dYQqoLwb5AGJASHqv3",
     "timestamp": 1620595978554
    },
    {
     "file_id": "1U_LAchbfb3T_4N40qQcJfUUSbpgTS61r",
     "timestamp": 1620209098555
    },
    {
     "file_id": "1qLV4gTUNJVNYuCJvMbGLc5gXpeZk2EIm",
     "timestamp": 1620203669488
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit33965378f01044d9b0dd8a18e6515429"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}