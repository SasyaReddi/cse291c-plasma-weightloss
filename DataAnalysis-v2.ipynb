{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh_FkgR00zpY"
   },
   "source": [
    "### <font color='blue'>**Data Exploration + XGBoost starter model**\n",
    "    Pulled from Eric's notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3912,
     "status": "ok",
     "timestamp": 1620771624579,
     "user": {
      "displayName": "Sasya Reddi",
      "photoUrl": "",
      "userId": "02400148251831989501"
     },
     "user_tz": 420
    },
    "id": "T6f2dxrBuR1z"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10350,
     "status": "ok",
     "timestamp": 1620771631080,
     "user": {
      "displayName": "Sasya Reddi",
      "photoUrl": "",
      "userId": "02400148251831989501"
     },
     "user_tz": 420
    },
    "id": "XdLCLw1wuR19"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, pep_df, min_timepoints_per_patient=2):\n",
    "        '''\n",
    "        pep_df should be the pep_reduced_intensity_df we created above\n",
    "        This dataset will only include patients with at least `min_timepoints_per_patient` timepoints\n",
    "        '''\n",
    "        self.pep_df = pep_df\n",
    "        \n",
    "        self.data = {} # patient id -> [array of timepoint vectors]\n",
    "        for patient_idx in range(1, 58+1):\n",
    "            patient_timepoint_vectors = []\n",
    "            for timepoint in range(1, 7+1):\n",
    "                try:\n",
    "                    # We replace all nans with zeros using np.nan_to_num\n",
    "                    patient_timepoint_vector = np.nan_to_num(self.pep_df[f\"Patient_{patient_idx:02d}.Timepoint_{timepoint:01d}\"].values)\n",
    "                    patient_timepoint_vectors.append(patient_timepoint_vector)\n",
    "                except:\n",
    "                    # This patient timepoint doesnt exist in the data\n",
    "                    pass\n",
    "            \n",
    "            # If this patient had at least `min_timepoints_per_patient` timepoints, then include this patient id\n",
    "            if len(patient_timepoint_vectors) >= min_timepoints_per_patient:\n",
    "                self.data[patient_idx] = patient_timepoint_vectors\n",
    "                \n",
    "    @staticmethod\n",
    "    def prepare_df(filepath):\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "        \n",
    "        # only keep the Patient_x.Timepoint_y formatted columns\n",
    "        df = df.iloc[:, :667]\n",
    "        \n",
    "        # Remove all _unmod columns\n",
    "        df = df[df.columns.drop(list(df.filter(regex='.\\_unmod')))]\n",
    "        \n",
    "        # Convert the string abundance numbers into ints for all patient timepoint cols\n",
    "        def convert_to_number(val):\n",
    "            if isinstance(val, str):\n",
    "                return int(val.replace(\",\",\"\").strip())\n",
    "            return val\n",
    "\n",
    "        for patient_timepoint_col in df.columns.values[32:]:\n",
    "            df[patient_timepoint_col] = df[patient_timepoint_col].apply(convert_to_number)\n",
    "            df[patient_timepoint_col] = df[patient_timepoint_col].astype(float)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, filepath, *args, **kwargs):\n",
    "        pip_df = cls.prepare_df(filepath)\n",
    "        return cls(pip_df, *args, **kwargs)\n",
    "        \n",
    "                \n",
    "    def data_generator(self, patient_ids, num_samples, peptide_indices=None, same_patient_pair_probability=0.5):\n",
    "        \"\"\"\n",
    "        This will return a generator that will yield `num_samples` sample pairs of peptide vectors \n",
    "        (limited to only the `patient_ids` given, and the `peptide_indices` given (if none is given, then all peptides will be included))\n",
    "        such that, `same_patient_pair_probability` of the time, the pair of vectors will be from the same patient (but different time points)\n",
    "        \n",
    "        Note: when same_patient_pair_probability is None, it will model the true data distribution by putting all timepoint vectors into 1 bucket\n",
    "        and randomly sampling from this (this will make it such that it is always way more likely for the timepoint vectors to be from different patients than the same one)\n",
    "        \n",
    "        Data samples yielded by the returned generator will be of the form:\n",
    "        {\n",
    "            \"first_patient_idx\" : ...,\n",
    "            \"first_patient_timepoint_vector\" : ...,\n",
    "            \"second_patient_idx\" : ...,\n",
    "            \"second_patient_timepoint_vector\" : ...,\n",
    "            \"is_same_patient\" : True/False\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # First ensure all of the given patient ids are in out prepared dataset\n",
    "        assert all(pid in self.data for pid in patient_ids), \"Not all of the given patient ids are in our dataset\"\n",
    "        \n",
    "        \n",
    "        def true_data_dist_gen():\n",
    "            # This generator is used when same_patient_pair_probability is None\n",
    "            all_patient_timepoint_vectors = []\n",
    "            for patient_idx in patient_ids:\n",
    "                for patient_timepoint_vector in self.data[patient_idx]:\n",
    "                    all_patient_timepoint_vectors.append((patient_idx, patient_timepoint_vector))\n",
    "            \n",
    "            for _ in range(num_samples):\n",
    "                first_patient_id, first_patient_timepoint_vector = random.choice(all_patient_timepoint_vectors)\n",
    "                second_patient_id, second_patient_timepoint_vector = random.choice(all_patient_timepoint_vectors)\n",
    "                \n",
    "                # if peptide_indices is given, filter the vectors to only include the `peptide_indices` peptides\n",
    "                if peptide_indices:\n",
    "                    first_patient_timepoint_vector = first_patient_timepoint_vector[peptide_indices]\n",
    "                    second_patient_timepoint_vector = second_patient_timepoint_vector[peptide_indices]\n",
    "                    \n",
    "                yield {\n",
    "                    \"first_patient_id\" : first_patient_id,\n",
    "                    \"first_patient_timepoint_vector\" : first_patient_timepoint_vector,\n",
    "                    \"second_patient_id\" : second_patient_id,\n",
    "                    \"second_patient_timepoint_vector\" : second_patient_timepoint_vector,\n",
    "                    \"is_same_patient\" : first_patient_id == second_patient_id\n",
    "                }\n",
    "\n",
    "        \n",
    "        def data_gen():\n",
    "            # This generator is used when same_patient_pair_probability is NOT None\n",
    "            for _ in range(num_samples):\n",
    "                \n",
    "                # pick a random first patient id\n",
    "                first_patient_id = random.choice(patient_ids)\n",
    "                \n",
    "                # pick a random timepoint vector for this first patient id\n",
    "                first_patient_timepoint_vector = random.choice(self.data[first_patient_id])\n",
    "                \n",
    "                # decide if the second patient in our pair should be the same as the first\n",
    "                is_second_patient_same = random.random() <= same_patient_pair_probability\n",
    "                \n",
    "                if is_second_patient_same:\n",
    "                    second_patient_id = first_patient_id\n",
    "                else:\n",
    "                    second_patient_id = random.choice([pid for pid in patient_ids if pid != first_patient_id])\n",
    "                \n",
    "                # Note: In this approach, it is possible that, when the second patient = first patient,\n",
    "                # They both return the same timepoint vector. but that should be okay, since this will help the model learn\n",
    "                # to detect identical inputs, especially when the input to the model is the concatenated vector\n",
    "                second_patient_timepoint_vector = random.choice(self.data[second_patient_id])\n",
    "                \n",
    "                # if peptide_indices is given, filter the vectors to only include the `peptide_indices` peptides\n",
    "                if peptide_indices:\n",
    "                    first_patient_timepoint_vector = first_patient_timepoint_vector[peptide_indices]\n",
    "                    second_patient_timepoint_vector = second_patient_timepoint_vector[peptide_indices]\n",
    "                \n",
    "                yield {\n",
    "                    \"first_patient_id\" : first_patient_id,\n",
    "                    \"first_patient_timepoint_vector\" : first_patient_timepoint_vector,\n",
    "                    \"second_patient_id\" : second_patient_id,\n",
    "                    \"second_patient_timepoint_vector\" : second_patient_timepoint_vector,\n",
    "                    \"is_same_patient\" : first_patient_id == second_patient_id\n",
    "                }\n",
    "\n",
    "        # Based on same_patient_pair_probability, return the correct data generator\n",
    "        if same_patient_pair_probability is None:\n",
    "            return true_data_dist_gen()\n",
    "        else:\n",
    "            return data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./Data\"\n",
    "DATA_TSV_FILEPATH = os.path.join(DATA_DIR, \"peptidoforms_intensity\", \"data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish all random seeds\n",
    "random.seed(2021)\n",
    "\n",
    "# TODO: other libraries should be seeded here, like sklearn, torch, xgboost, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>**Creating Main Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subby/UCSD/CSE_291C/Projects/cse291c-plasma-weightloss/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (38,42,44,48,52,56,92,98,102,108,116,122,124,126,128,130,132,134,138,140,144,146,148,150,152,154,156,176,184,190,196,200,212,216,228,234,236,238,242,244,246,248,250,252,254,256,258,260,262,264,268,270,272,274,284,286,288,290,292,294,296,298,300,302,304,306,308,310,312,314,316,318,322,324,330,332,334,336,338,340,342,344,346,350,364,372,374,378,380,382,388,390,392,394,398,404,406,408,410,412,414,416,426,428,440,444,446,454,458,460,464,466,468,484,502,510,514,516,522,524,526,528,538,540,542,544,552,554,556,558,562,576,592,606,608,610,612,618,622,624,626,628,630,632,640,642,644,646,650,652,654,656,658,660,662,664,674,676,678,684,686,692,694,696,698,700,702) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "peptidoforms_intensity_dataset = Dataset.from_file(DATA_TSV_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40921, 350)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptidoforms_intensity_df = peptidoforms_intensity_dataset.pep_df\n",
    "peptidoforms_intensity_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([318, 318, 318, ...,   1,   7,   1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each peptide index, count the number of NON-NaNs it has across all patient timepoints\n",
    "peptide_nonempty_count_across_patient_timepoints = np.count_nonzero(~np.isnan(peptidoforms_intensity_df.values[:,32:].astype(float)), axis=-1)\n",
    "peptide_nonempty_count_across_patient_timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40921.000000\n",
       "mean        25.752572\n",
       "std         68.130665\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          8.000000\n",
       "max        318.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(peptide_nonempty_count_across_patient_timepoints).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37461"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(peptide_nonempty_count_across_patient_timepoints[peptide_nonempty_count_across_patient_timepoints <= 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6Z8H_TkcxZE"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFUA7k1jGkcH"
   },
   "source": [
    "### <font color='blue'>**Creating Train/Val/Test Datasets**\n",
    "    \n",
    "Train and val datasets will enforce the `same_patient_pair_probability = 0.5`, while the test set will use the true data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we don't select a particular subset of peptides, and just use all of them.\n",
    "selected_peptide_indices = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_patient_pair_probability = 0.5\n",
    "val_same_patient_pair_probability = 0.5\n",
    "test_same_patient_pair_probability = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 1k samples for train/val/test finishes fast, but may not be accurate\n",
    "# so we should train with more samples (~100K train/val/test) with colab/gpu later, which would take more time\n",
    "num_train_samples = 10000\n",
    "num_val_samples = 10000\n",
    "num_test_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_patient_ids = list(peptidoforms_intensity_dataset.data.keys())\n",
    "num_patient_ids = len(all_patient_ids)\n",
    "num_patient_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 10, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(all_patient_ids)\n",
    "train_patient_ids, val_patient_ids, test_patient_ids = np.split(all_patient_ids, \n",
    "                                                                [int(train_ratio * num_patient_ids), \n",
    "                                                                 int((train_ratio + val_ratio) * num_patient_ids)\n",
    "                                                                ])\n",
    "train_patient_ids = train_patient_ids.tolist()\n",
    "val_patient_ids = val_patient_ids.tolist()\n",
    "test_patient_ids = test_patient_ids.tolist()\n",
    "\n",
    "len(train_patient_ids), len(val_patient_ids), len(test_patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_arrays_from_generator(gen):\n",
    "    \"\"\"\n",
    "    The input is a concatenated vector of the 2 patient vectors, and the output is 1/0 if the vectors are from the same patient or not\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for data_input in tqdm(gen):\n",
    "        x = np.hstack((data_input['first_patient_timepoint_vector'], data_input['second_patient_timepoint_vector'])).astype(int)\n",
    "        y = 1 if data_input['is_same_patient'] else 0\n",
    "        \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:04, 2332.82it/s]\n",
      "10000it [00:05, 1692.23it/s]\n",
      "10000it [00:06, 1506.25it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = peptidoforms_intensity_dataset.data_generator(\n",
    "    patient_ids=train_patient_ids, \n",
    "    num_samples=num_train_samples, \n",
    "    peptide_indices=selected_peptide_indices, \n",
    "    same_patient_pair_probability=train_same_patient_pair_probability)\n",
    "trainX, trainY = create_data_arrays_from_generator(train_data_gen)\n",
    "\n",
    "val_data_gen = peptidoforms_intensity_dataset.data_generator(\n",
    "    patient_ids=val_patient_ids, \n",
    "    num_samples=num_val_samples, \n",
    "    peptide_indices=selected_peptide_indices, \n",
    "    same_patient_pair_probability=val_same_patient_pair_probability)\n",
    "valX, valY = create_data_arrays_from_generator(val_data_gen)\n",
    "\n",
    "test_data_gen = peptidoforms_intensity_dataset.data_generator(\n",
    "    patient_ids=test_patient_ids, \n",
    "    num_samples=num_test_samples, \n",
    "    peptide_indices=selected_peptide_indices, \n",
    "    same_patient_pair_probability=test_same_patient_pair_probability)\n",
    "testX, testY = create_data_arrays_from_generator(test_data_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7AWhg0RJXeQ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE on using scalers: \n",
    "Current approach of just using standard scaling on concatenated input feature vectors is likely not optimal/correct.\n",
    "\n",
    "This is because, the first half and the second half of each concatenated feature vector is semantically the same, yet the scaler will scale them differently.\n",
    "\n",
    "A better approach may be to create a data matrix of shape (num peptide, num patient time points) (and replacing all NaNs with 0s), and fit the standard scaler on this (using only the training patient ids). Then, when transforming the data, we transform the first half of each vector and the second half of each vector separately (using the same scaler), and concat these 2 scaled halfs to create a new scaled & concatenated feature vector\n",
    "\n",
    "However, tree-based methods (including XGBoost) are invariant to feature scaling, so they don't need any scaling: https://datascience.stackexchange.com/questions/60950/is-it-necessary-to-normalise-data-for-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJAvFabadQuR"
   },
   "source": [
    "### <font color='blue'>**XGBoost Classifier**\n",
    "Initial log loss should be around $0.69$, since $-ln(0.5) = 0.6931471806$ and there is a $0.5$ probability of same/different patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_dataX = np.array(trainX) # scaler.transform(trainX)\n",
    "val_dataX = np.array(valX) # scaler.transform(valX)\n",
    "test_dataX = np.array(testX) # scaler.transform(testX)\n",
    "\n",
    "train_dataY = np.array(trainY)\n",
    "val_dataY = np.array(valY)\n",
    "test_dataY = np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5005\n",
       "1    4995\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataY).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5079\n",
       "0    4921\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(val_dataY).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8954\n",
       "1    1046\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_dataY).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60095,
     "status": "ok",
     "timestamp": 1620771681210,
     "user": {
      "displayName": "Sasya Reddi",
      "photoUrl": "",
      "userId": "02400148251831989501"
     },
     "user_tz": 420
    },
    "id": "DNyMiM4BQib8",
    "outputId": "7f3ce4a0-f7ee-4768-b271-4650aa90f15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.68995\n",
      "[1]\tvalidation_0-logloss:0.67595\n",
      "[2]\tvalidation_0-logloss:0.67716\n",
      "[3]\tvalidation_0-logloss:0.66137\n",
      "[4]\tvalidation_0-logloss:0.64409\n",
      "[5]\tvalidation_0-logloss:0.64687\n",
      "[6]\tvalidation_0-logloss:0.64744\n",
      "[7]\tvalidation_0-logloss:0.64738\n",
      "[8]\tvalidation_0-logloss:0.64494\n",
      "[9]\tvalidation_0-logloss:0.59304\n",
      "[10]\tvalidation_0-logloss:0.59140\n",
      "[11]\tvalidation_0-logloss:0.57336\n",
      "[12]\tvalidation_0-logloss:0.56571\n",
      "[13]\tvalidation_0-logloss:0.56120\n",
      "[14]\tvalidation_0-logloss:0.56080\n",
      "[15]\tvalidation_0-logloss:0.54087\n",
      "[16]\tvalidation_0-logloss:0.52551\n",
      "[17]\tvalidation_0-logloss:0.50876\n",
      "[18]\tvalidation_0-logloss:0.50779\n",
      "[19]\tvalidation_0-logloss:0.48774\n",
      "[20]\tvalidation_0-logloss:0.48405\n",
      "[21]\tvalidation_0-logloss:0.48512\n",
      "[22]\tvalidation_0-logloss:0.48524\n",
      "[23]\tvalidation_0-logloss:0.47938\n",
      "[24]\tvalidation_0-logloss:0.48048\n",
      "[25]\tvalidation_0-logloss:0.48005\n",
      "[26]\tvalidation_0-logloss:0.48052\n",
      "[27]\tvalidation_0-logloss:0.46323\n",
      "[28]\tvalidation_0-logloss:0.46518\n",
      "[29]\tvalidation_0-logloss:0.45676\n",
      "[30]\tvalidation_0-logloss:0.45545\n",
      "[31]\tvalidation_0-logloss:0.42957\n",
      "[32]\tvalidation_0-logloss:0.42030\n",
      "[33]\tvalidation_0-logloss:0.41936\n",
      "[34]\tvalidation_0-logloss:0.41396\n",
      "[35]\tvalidation_0-logloss:0.41368\n",
      "[36]\tvalidation_0-logloss:0.41076\n",
      "[37]\tvalidation_0-logloss:0.40752\n",
      "[38]\tvalidation_0-logloss:0.37638\n",
      "[39]\tvalidation_0-logloss:0.38079\n",
      "[40]\tvalidation_0-logloss:0.36690\n",
      "[41]\tvalidation_0-logloss:0.36451\n",
      "[42]\tvalidation_0-logloss:0.35471\n",
      "[43]\tvalidation_0-logloss:0.35409\n",
      "[44]\tvalidation_0-logloss:0.35154\n",
      "[45]\tvalidation_0-logloss:0.35483\n",
      "[46]\tvalidation_0-logloss:0.35667\n",
      "[47]\tvalidation_0-logloss:0.35699\n",
      "[48]\tvalidation_0-logloss:0.35386\n",
      "[49]\tvalidation_0-logloss:0.34487\n",
      "[50]\tvalidation_0-logloss:0.34323\n",
      "[51]\tvalidation_0-logloss:0.34173\n",
      "[52]\tvalidation_0-logloss:0.34176\n",
      "[53]\tvalidation_0-logloss:0.32877\n",
      "[54]\tvalidation_0-logloss:0.31688\n",
      "[55]\tvalidation_0-logloss:0.30617\n",
      "[56]\tvalidation_0-logloss:0.32648\n",
      "[57]\tvalidation_0-logloss:0.32146\n",
      "[58]\tvalidation_0-logloss:0.30171\n",
      "[59]\tvalidation_0-logloss:0.30174\n",
      "[60]\tvalidation_0-logloss:0.29987\n",
      "[61]\tvalidation_0-logloss:0.30416\n",
      "[62]\tvalidation_0-logloss:0.30569\n",
      "[63]\tvalidation_0-logloss:0.30891\n",
      "[64]\tvalidation_0-logloss:0.30728\n",
      "[65]\tvalidation_0-logloss:0.30649\n",
      "[66]\tvalidation_0-logloss:0.30810\n",
      "[67]\tvalidation_0-logloss:0.29990\n",
      "[68]\tvalidation_0-logloss:0.29875\n",
      "[69]\tvalidation_0-logloss:0.29906\n",
      "[70]\tvalidation_0-logloss:0.28670\n",
      "[71]\tvalidation_0-logloss:0.28104\n",
      "[72]\tvalidation_0-logloss:0.28092\n",
      "[73]\tvalidation_0-logloss:0.28061\n",
      "[74]\tvalidation_0-logloss:0.28008\n",
      "[75]\tvalidation_0-logloss:0.27011\n",
      "[76]\tvalidation_0-logloss:0.26825\n",
      "[77]\tvalidation_0-logloss:0.26653\n",
      "[78]\tvalidation_0-logloss:0.26358\n",
      "[79]\tvalidation_0-logloss:0.26039\n",
      "[80]\tvalidation_0-logloss:0.24200\n",
      "[81]\tvalidation_0-logloss:0.22989\n",
      "[82]\tvalidation_0-logloss:0.23208\n",
      "[83]\tvalidation_0-logloss:0.22346\n",
      "[84]\tvalidation_0-logloss:0.22070\n",
      "[85]\tvalidation_0-logloss:0.22028\n",
      "[86]\tvalidation_0-logloss:0.21007\n",
      "[87]\tvalidation_0-logloss:0.20306\n",
      "[88]\tvalidation_0-logloss:0.20042\n",
      "[89]\tvalidation_0-logloss:0.19195\n",
      "[90]\tvalidation_0-logloss:0.19585\n",
      "[91]\tvalidation_0-logloss:0.18945\n",
      "[92]\tvalidation_0-logloss:0.18357\n",
      "[93]\tvalidation_0-logloss:0.18068\n",
      "[94]\tvalidation_0-logloss:0.17969\n",
      "[95]\tvalidation_0-logloss:0.18593\n",
      "[96]\tvalidation_0-logloss:0.19011\n",
      "[97]\tvalidation_0-logloss:0.18945\n",
      "[98]\tvalidation_0-logloss:0.18279\n",
      "[99]\tvalidation_0-logloss:0.17018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(use_label_encoder=False, max_depth=3)\n",
    "model.fit(train_dataX, train_dataY, eval_metric=\"logloss\", eval_set=[(val_dataX, val_dataY)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[5005    0]\n",
      " [   0 4995]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5005\n",
      "           1       1.00      1.00      1.00      4995\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n",
      "Train accuracy -  1.0\n"
     ]
    }
   ],
   "source": [
    "# Train metrics\n",
    "train_ypred = model.predict(train_dataX)\n",
    "train_predictions = [round(value) for value in train_ypred]\n",
    "\n",
    "metrics(train_dataY, train_predictions)\n",
    "train_accuracy = accuracy_score(train_dataY, train_predictions)\n",
    "\n",
    "print(\"Train accuracy - \", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[4904   17]\n",
      " [ 525 4554]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95      4921\n",
      "           1       1.00      0.90      0.94      5079\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n",
      "Val accuracy -  0.9458\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics\n",
    "val_ypred = model.predict(val_dataX)\n",
    "val_predictions = [round(value) for value in val_ypred]\n",
    "\n",
    "metrics(val_dataY, val_predictions)\n",
    "val_accuracy = accuracy_score(val_dataY, val_predictions)\n",
    "\n",
    "print(\"Val accuracy - \", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[8952    2]\n",
      " [  91  955]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      8954\n",
      "           1       1.00      0.91      0.95      1046\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.96      0.97     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Test accuracy -  0.9907\n"
     ]
    }
   ],
   "source": [
    "# Test metrics\n",
    "test_ypred = model.predict(test_dataX)\n",
    "test_predictions = [round(value) for value in test_ypred]\n",
    "\n",
    "metrics(test_dataY, test_predictions)\n",
    "test_accuracy = accuracy_score(test_dataY, test_predictions)\n",
    "\n",
    "print(\"Test accuracy - \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = model.feature_importances_\n",
    "len(ft[ft > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     3,     4,    10,    12,    15,    18,    57,    58,\n",
       "         113,   126,   143,   184,   206,   276,   301,   340,   373,\n",
       "         400,   406,   496,   499,   588,   599,   603,   607,   608,\n",
       "         614,   634,   667,   687,   696,   699,   713,   757,   779,\n",
       "         820,   896,   913,   956,   978,   983,   991,  1054,  1068,\n",
       "        1071,  1098,  1119,  1171,  1193,  1194,  1203,  1220,  1232,\n",
       "        1351,  1398,  1471,  1546,  1633,  1664,  1722,  1729,  1757,\n",
       "        1783,  1805,  1809,  1817,  1822,  1837,  1865,  1913,  1948,\n",
       "        1966,  1969,  1970,  2093,  2097,  2108,  2139,  2157,  2161,\n",
       "        2198,  2230,  2242,  2295,  2302,  2313,  2317,  2323,  2354,\n",
       "        2357,  2361,  2401,  2482,  2493,  2534,  2583,  2598,  2649,\n",
       "        2701,  2730,  2760,  2849,  2890,  2903,  2945,  2969,  2977,\n",
       "        2981,  2998,  3026,  3035,  3040,  3069,  3158,  3162,  3271,\n",
       "        3304,  3394,  3410,  3418,  3426,  3557,  3572,  3585,  3615,\n",
       "        3717,  3726,  3747,  3800,  3831,  3910,  3920,  3942,  3989,\n",
       "        3996,  4014,  4031,  4061,  4086,  4113,  4125,  4166,  4227,\n",
       "        4232,  4298,  4362,  4369,  4412,  4417,  4466,  4472,  4505,\n",
       "        4567,  4606,  4613,  4636,  4639,  4657,  4676,  4722,  4800,\n",
       "        4812,  4827,  4855,  4860,  4923,  4953,  4954,  4957,  5024,\n",
       "        5136,  5158,  5216,  5276,  5313,  5389,  5441,  5499,  5530,\n",
       "        5587,  5589,  5618,  5626,  5638,  5686,  5687,  5703,  5858,\n",
       "        5895,  5916,  5922,  5951,  6228,  6270,  6280,  6455,  6582,\n",
       "        6645,  6825,  6878,  6920,  7029,  7130,  7241,  7246,  7297,\n",
       "        7323,  7440,  7457,  7579,  7619,  7622,  7658,  7682,  7683,\n",
       "        7732,  7745,  7747,  7816,  7826,  8077,  8315,  8550,  8651,\n",
       "        8848,  8856,  9197,  9272,  9374,  9543,  9579,  9633,  9975,\n",
       "       10006, 10016, 10151, 10341, 10474, 10849, 10999, 11252, 11556,\n",
       "       11819, 11983, 12201, 12429, 12526, 13364, 14250, 14412, 14758,\n",
       "       14778, 15318, 15547, 15866, 15918, 16763, 17617, 17932, 18648,\n",
       "       19871, 20795, 21316, 21630, 21913, 23895, 24899, 26966, 27412,\n",
       "       40921, 40926, 40927, 40935, 40944, 40948, 40963, 40971, 41026,\n",
       "       41039, 41042, 41052, 41064, 41065, 41104, 41180, 41197, 41227,\n",
       "       41253, 41254, 41255, 41261, 41294, 41311, 41327, 41365, 41371,\n",
       "       41415, 41424, 41457, 41529, 41535, 41542, 41555, 41568, 41588,\n",
       "       41590, 41628, 41678, 41679, 41738, 41744, 41751, 41806, 41853,\n",
       "       41877, 41912, 41949, 41954, 41975, 42022, 42053, 42094, 42114,\n",
       "       42138, 42153, 42184, 42209, 42242, 42264, 42284, 42304, 42305,\n",
       "       42333, 42370, 42383, 42424, 42467, 42468, 42554, 42596, 42631,\n",
       "       42678, 42726, 42730, 42732, 42777, 42813, 42827, 42850, 42887,\n",
       "       42890, 42891, 42926, 43018, 43022, 43029, 43044, 43151, 43163,\n",
       "       43169, 43216, 43223, 43244, 43278, 43282, 43374, 43414, 43423,\n",
       "       43436, 43440, 43496, 43504, 43578, 43610, 43619, 43638, 43660,\n",
       "       43725, 43759, 43770, 43824, 43855, 43866, 43890, 43947, 43956,\n",
       "       43961, 44010, 44066, 44079, 44115, 44126, 44257, 44315, 44323,\n",
       "       44339, 44410, 44422, 44424, 44506, 44516, 44518, 44526, 44531,\n",
       "       44536, 44564, 44603, 44638, 44668, 44691, 44809, 44841, 44863,\n",
       "       44910, 44935, 45007, 45034, 45053, 45068, 45087, 45089, 45146,\n",
       "       45148, 45153, 45166, 45219, 45256, 45283, 45290, 45333, 45338,\n",
       "       45387, 45404, 45420, 45426, 45527, 45534, 45541, 45560, 45643,\n",
       "       45721, 45748, 45776, 45781, 45839, 45867, 45926, 46031, 46079,\n",
       "       46092, 46130, 46137, 46226, 46227, 46234, 46241, 46262, 46265,\n",
       "       46362, 46469, 46510, 46559, 46567, 46609, 46624, 46672, 46779,\n",
       "       46872, 47146, 47201, 47272, 47300, 47401, 47516, 47520, 47623,\n",
       "       47709, 47746, 47950, 48162, 48180, 48188, 48218, 48550, 48642,\n",
       "       48698, 48731, 48747, 49070, 49206, 49421, 49482, 49572, 49670,\n",
       "       49719, 49769, 49777, 49807, 49948, 50118, 50163, 50193, 50207,\n",
       "       50554, 50766, 50896, 51472, 51538, 52378, 52477, 52627, 52740,\n",
       "       52880, 52904, 52927, 53232, 53288, 53447, 53881, 53898, 54350,\n",
       "       55668, 55670, 55699, 56100, 56239, 56414, 56468, 57034, 57678,\n",
       "       57684, 58937, 60010, 60792, 61676, 61751, 67800, 67887, 72595])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_peptide_indices = np.where(ft > 0)[0]\n",
    "important_peptide_indices"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sasya (+PCA) - MLApproach.ipynb",
   "provenance": [
    {
     "file_id": "12_DObUJrtJgTxoDAO0EMkmFffF00QZW-",
     "timestamp": 1620702652816
    },
    {
     "file_id": "1pNQiIjpJYOh77_dYQqoLwb5AGJASHqv3",
     "timestamp": 1620595978554
    },
    {
     "file_id": "1U_LAchbfb3T_4N40qQcJfUUSbpgTS61r",
     "timestamp": 1620209098555
    },
    {
     "file_id": "1qLV4gTUNJVNYuCJvMbGLc5gXpeZk2EIm",
     "timestamp": 1620203669488
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
